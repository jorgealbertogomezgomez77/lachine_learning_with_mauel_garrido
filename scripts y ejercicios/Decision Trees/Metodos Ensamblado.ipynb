{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-07T07:34:17-05:00\n",
      "\n",
      "CPython 3.7.3rc1\n",
      "IPython 7.3.0\n",
      "\n",
      "compiler   : MSC v.1916 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(datos.data, columns=datos.feature_names)\n",
    "boston[\"objetivo\"] = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>objetivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  objetivo  \n",
       "0     15.3  396.90   4.98      24.0  \n",
       "1     17.8  396.90   9.14      21.6  \n",
       "2     17.8  392.83   4.03      34.7  \n",
       "3     18.7  394.63   2.94      33.4  \n",
       "4     18.7  396.90   5.33      36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INCISO**\n",
    "\n",
    "Hasta ahora hemos usado la magia de jupyter notebook `?` y `??` para ver cual es la documentacion de una clase o función. Jupyter lo unico que hace es leer el docstring de dichos objetos. El docstring de un objeto en python es el string delimitado por `\"\"\"` que se define justo después del nombre de dicho objeto, y tiene como objetivo el documentar el uso del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si creamos la clase `ClaseTest` con un docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaseTest():\n",
    "    \"\"\"Éste es el docstring\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos usar la magia de jupyter para que nos imprima el docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClaseTest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos imprimir directamente el atributo `__doc__` de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éste es el docstring\n"
     ]
    }
   ],
   "source": [
    "print(ClaseTest.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a evaluar los algoritmos que conocemos hasta ahora y compararlos con los distintos algoritmos de ensamblado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_cv(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return rmse(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.565069432592691,\n",
       " 'elasticnet': 5.261057069533587,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969341}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "estimador_arbol = DecisionTreeRegressor()\n",
    "\n",
    "error_cv = cross_val_score(estimador_arbol, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"arbol\"] = error_cv\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "\n",
    "estimador_elnet = ElasticNet()\n",
    "\n",
    "resultados[\"elasticnet\"] = cross_val_score(estimador_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "\n",
    "estimador_lasso = Lasso()\n",
    "estimador_ridge = Ridge()\n",
    "\n",
    "\n",
    "resultados[\"lasso\"] = cross_val_score(estimador_lasso, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"ridge\"] = cross_val_score(estimador_ridge, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Bagging (Bootstrap aggregating) funcionan entrenando varios estimadores base y cambiando los datos de entrenamiento para cada uno. En sklearn los algoritmos de ensamblado de modelos se encuentran en el submódulo `sklearn.ensemble`. En cuanto a Bagging, sklearn tiene una versión para problemas de regresión (`BaggingRegressor`) y otra para problemas de clasificación (`BaggingClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bagging regressor.\n",
      "\n",
      "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "    regressors each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions (either by voting or by averaging)\n",
      "    to form a final prediction. Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "    tree), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "    This algorithm encompasses several works from the literature. When random\n",
      "    subsets of the dataset are drawn as random subsets of the samples, then\n",
      "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "    of the dataset are drawn as random subsets of the features, then the method\n",
      "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "    on subsets of both samples and features, then the method is known as\n",
      "    Random Patches [4]_.\n",
      "\n",
      "    Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, optional (default=None)\n",
      "        The base estimator to fit on random subsets of the dataset.\n",
      "        If None, then the base estimator is a decision tree.\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    max_features : int or float, optional (default=1.0)\n",
      "        The number of features to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_features` features.\n",
      "        - If float, then draw `max_features * X.shape[1]` features.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether samples are drawn with replacement. If False, sampling\n",
      "        without replacement is performed.\n",
      "\n",
      "    bootstrap_features : boolean, optional (default=False)\n",
      "        Whether features are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to True, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit\n",
      "        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of estimators\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "    estimators_features_ : list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_prediction_` might contain NaN.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "           1996.\n",
      "\n",
      "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "           1998.\n",
      "\n",
      "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BaggingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.324085210844261"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_10 = BaggingRegressor(n_estimators=10)\n",
    "error_cv = cross_val_score(estimador_bagging_10, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_10\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentar el número de estimadores base es una forma limitada pero sencilla de mejorar el funcionamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.139356094305253"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_100 = BaggingRegressor(n_estimators=100)\n",
    "error_cv = cross_val_score(estimador_bagging_100, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaggingRegressor` utiliza árboles de decisión como estimador base por defecto, sin embargo podemos utilizar uno distinto mediante el parámetro `base_estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.272961905392164"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ElasticNet())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_elnet\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su momento vimos que existe un tipo de arbol de decision completamente aleatorio (Extremely Randomized Trees) que deciden la particion en cada nodo al azar. Vemos que al agrupar muchos de estos estimadores que son débiles (aun que mejores que tirar una moneda al azar, ya que un árbol de decision aleatorio aun asi aprende a separar los elementos), la varianza general se reduce ya que la que aporta un arbol se complementa con la del de al lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8602984269332885"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor\n",
    "\n",
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ExtraTreeRegressor())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_extra_arbol\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de boosting intentan mejorar los estimadores base asignando pesos en funcion de su funcionamiento individual. El algoritmo clásico de boosting es `AdaBoost`, que se encuentra en sklearn como `AdaBoostRegressor` y `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AdaBoost regressor.\n",
      "\n",
      "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "    regressor on the original dataset and then fits additional copies of the\n",
      "    regressor on the same dataset but where the weights of instances are\n",
      "    adjusted according to the error of the current prediction. As such,\n",
      "    subsequent regressors focus more on difficult cases.\n",
      "\n",
      "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "\n",
      "    Read more in the :ref:`User Guide <adaboost>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object, optional (default=None)\n",
      "        The base estimator from which the boosted ensemble is built.\n",
      "        Support for sample weighting is required. If ``None``, then\n",
      "        the base estimator is ``DecisionTreeRegressor(max_depth=3)``\n",
      "\n",
      "    n_estimators : integer, optional (default=50)\n",
      "        The maximum number of estimators at which boosting is terminated.\n",
      "        In case of perfect fit, the learning procedure is stopped early.\n",
      "\n",
      "    learning_rate : float, optional (default=1.)\n",
      "        Learning rate shrinks the contribution of each regressor by\n",
      "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "        ``n_estimators``.\n",
      "\n",
      "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "        The loss function to use when updating the weights after each\n",
      "        boosting iteration.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of classifiers\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimator_weights_ : array of floats\n",
      "        Weights for each estimator in the boosted ensemble.\n",
      "\n",
      "    estimator_errors_ : array of floats\n",
      "        Regression error for each estimator in the boosted ensemble.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances if supported by the ``base_estimator``.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    AdaBoostClassifier, GradientBoostingRegressor,\n",
      "    sklearn.tree.DecisionTreeRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "           on-Line Learning and an Application to Boosting\", 1995.\n",
      "\n",
      "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(AdaBoostRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.432473271209594"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_adaboost = AdaBoostRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_adaboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"adaboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro algoritmo de Boosting es Gradient Boosting que a cada iteración usa el algoritmo de Descenso de Gradiente (que veremos en el futuro) para a cada iteración , entrenar un estimador nuevo que minimiza la función de error (*loss function*) del modelo.\n",
    "\n",
    "Scikit-learn implementa el algoritmo de (Gradient Boosted Regression Trees), que usa árboles de decisión como estimadores base, en [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) y [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "Gradient Boosting puede usar cualquier funcion de error siempre que sea diferenciable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting for regression.\n",
      "\n",
      "    GB builds an additive model in a forward stage-wise fashion;\n",
      "    it allows for the optimization of arbitrary differentiable loss functions.\n",
      "    In each stage a regression tree is fit on the negative gradient of the\n",
      "    given loss function.\n",
      "\n",
      "    Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "        loss function to be optimized. 'ls' refers to least squares\n",
      "        regression. 'lad' (least absolute deviation) is a highly robust\n",
      "        loss function solely based on order information of the input\n",
      "        variables. 'huber' is a combination of the two. 'quantile'\n",
      "        allows quantile regression (use `alpha` to specify the quantile).\n",
      "\n",
      "    learning_rate : float, optional (default=0.1)\n",
      "        learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "        There is a trade-off between learning_rate and n_estimators.\n",
      "\n",
      "    n_estimators : int (default=100)\n",
      "        The number of boosting stages to perform. Gradient boosting\n",
      "        is fairly robust to over-fitting so a large number usually\n",
      "        results in better performance.\n",
      "\n",
      "    subsample : float, optional (default=1.0)\n",
      "        The fraction of samples to be used for fitting the individual base\n",
      "        learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "        Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "        Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "    criterion : string, optional (default=\"friedman_mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"friedman_mse\" for the mean squared error with improvement\n",
      "        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "        the mean absolute error. The default value of \"friedman_mse\" is\n",
      "        generally the best as it can provide a better approximation in\n",
      "        some cases.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_depth : integer, optional (default=3)\n",
      "        maximum depth of the individual regression estimators. The maximum\n",
      "        depth limits the number of nodes in the tree. Tune this parameter\n",
      "        for best performance; the best value depends on the interaction\n",
      "        of the input variables.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    min_impurity_split : float, (default=1e-7)\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19. The default value of\n",
      "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    init : estimator, optional (default=None)\n",
      "        An estimator object that is used to compute the initial\n",
      "        predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      "        If None it uses ``loss.init_estimator``.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=None)\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Choosing `max_features < n_features` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    alpha : float (default=0.9)\n",
      "        The alpha-quantile of the huber loss function and the quantile\n",
      "        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "\n",
      "    verbose : int, default: 0\n",
      "        Enable verbose output. If 1 then it prints progress and performance\n",
      "        once in a while (the more trees the lower the frequency). If greater\n",
      "        than 1 then it prints progress and performance for every tree.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    warm_start : bool, default: False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just erase the\n",
      "        previous solution. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    presort : bool or 'auto', optional (default='auto')\n",
      "        Whether to presort the data to speed up the finding of best splits in\n",
      "        fitting. Auto mode by default will use presorting on dense data and\n",
      "        default to normal sorting on sparse data. Setting presort to true on\n",
      "        sparse data will raise an error.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           optional parameter *presort*.\n",
      "\n",
      "    validation_fraction : float, optional, default 0.1\n",
      "        The proportion of training data to set aside as validation set for\n",
      "        early stopping. Must be between 0 and 1.\n",
      "        Only used if ``n_iter_no_change`` is set to an integer.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    n_iter_no_change : int, default None\n",
      "        ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "        to terminate training when validation score is not improving. By\n",
      "        default it is set to None to disable early stopping. If set to a\n",
      "        number, it will set aside ``validation_fraction`` size of the training\n",
      "        data as validation and terminate training when validation score is not\n",
      "        improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "        iterations.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    tol : float, optional, default 1e-4\n",
      "        Tolerance for the early stopping. When the loss is not improving\n",
      "        by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "        number), the training stops.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    feature_importances_ : array, shape (n_features,)\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    oob_improvement_ : array, shape (n_estimators,)\n",
      "        The improvement in loss (= deviance) on the out-of-bag samples\n",
      "        relative to the previous iteration.\n",
      "        ``oob_improvement_[0]`` is the improvement in\n",
      "        loss of the first stage over the ``init`` estimator.\n",
      "\n",
      "    train_score_ : array, shape (n_estimators,)\n",
      "        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "        model at iteration ``i`` on the in-bag sample.\n",
      "        If ``subsample == 1`` this is the deviance on the training data.\n",
      "\n",
      "    loss_ : LossFunction\n",
      "        The concrete ``LossFunction`` object.\n",
      "\n",
      "    init_ : estimator\n",
      "        The estimator that provides the initial predictions.\n",
      "        Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "\n",
      "    estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data and\n",
      "    ``max_features=n_features``, if the improvement of the criterion is\n",
      "    identical for several splits enumerated during the search of the best\n",
      "    split. To obtain a deterministic behaviour during fitting,\n",
      "    ``random_state`` has to be fixed.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, RandomForestRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "\n",
      "    J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "\n",
      "    T. Hastie, R. Tibshirani and J. Friedman.\n",
      "    Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(GradientBoostingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9452443022624912"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_gradientboost = GradientBoostingRegressor(n_estimators=100, loss='ls')\n",
    "\n",
    "error_cv = cross_val_score(estimador_gradientboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"gradientboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cualquier estimador basado en árboles, `GradientBoostRegressor` nos permite ver la importancia de las variables en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8VWW9x/HPNwQFUXDACY2ThFNmpGTdmxkOOZRjk5CVNKk3SzNp0CbUHCoLNU0vlaHdxHAMrTSvytVyBAFRFHNWnDVxIgf83T/Ws3Wx2ftwYJ291z5nf9+v136dtZ71rLV/62zYv/OstffvUURgZma2ot5WdgBmZtazOZGYmVkhTiRmZlaIE4mZmRXiRGJmZoU4kZiZWSFOJNarSXq7pBcl9elC39GSHulk+2RJP+7eCM16PicSaxmSrpB0TI32vSU9Lmml5T1mRDwUEQMjYnH3RLliJIWkd5YZQ4WkByTtXHYc1ns4kVgrmQx8TpKq2j8H/CEiXl+eg61I4unN/PuwRnEisVZyCbAm8KFKg6Q1gD2Ac9L6xyTNkvS8pIclTcj17Uh/+X9J0kPA1bm2lVKfL0i6U9ILku6TdFB1EJKOkvR0+st9/3rBStpD0mxJz0m6XtJWXTlJSRMknS/pf1IccyVtIulISU+m89ol13+6pBMk3SxpoaQ/SVozt30vSXekOKZL2jy37QFJ35F0G/CSpCnA24FL0yW/b6d+56dR30JJ10p6V+4YkyWdLunPKd6bJA3PbX+XpCslPSvpCUlHpfa3SfqupHslPSNpaj5u6z2cSKxlRMQiYCrw+Vzzp4G7ImJOWn8pbR8MfAz4L0n7VB3qw8DmwK41nuZJssS0OvAFYKKkrXPb1wPWBoYCBwCTJG1afZC0z1nAQcBawH8D0ySt3MXT3RP4PbAGMAu4guz/41DgmHS8vM8DXwQ2AF4HTk1xbAJMAb4BDAH+QpYk+uX2HUv2uxocEWOBh4A90yW/n6Y+fwVGAOsAtwJ/qHr+scDRKd57gOPS868G/C9weYrtncBVaZ9DgX3IXo8NgH8Bp3fx92M9SUT44UfLPIDtgIVA/7T+D+DwTvqfDExMyx1AABvntlfaVqqz/yXAYWl5NNmb9Kq57VOBH6TlycCP0/IZwLFVx5oPfLjO8wTwzrQ8Abgyt21P4EWgT1pfLfUfnNanAyfm+m8BvAr0AX4ATM1texuwABid1h8AvlgVywPAzp38Tgen5x+UO+/f5LZ/lCy5Q5ZgZtU5zp3ATrn19YHX6r0WfvTch0ck1lIi4u/AU8DekjYG3gecW9ku6f2SrpH0lKSFwMFkI4i8h+sdX9Lukm5Ml2GeI3tTzO//r4h4Kbf+INlf09WGAUeky0nPpWNtVKdvLU/klhcBT8dbHwhYlH4OzPXJn9ODQN8U9wZpHYCIeCP1HVpn36VI6iPpxHQJ6nmyRANL/l4ezy2/nIttI+DeOoceBlyc+/3cCSwG1u0sHut5nEisFZ1Ddinnc8DfIiL/pnsuMA3YKCIGAWcC1Tfna5a0TpedLgROAtaNiMFkl4Ly+68hadXc+tuBR2sc7mHguIgYnHsMiIgpXT7L5bNRVUyvAU+n2IZVNqQPKmxENiqpqP59VK9/Btgb2BkYRDaKg6V/r7U8DAzvZNvuVb+jVSJiQZ3+1kM5kVgrOofsTe0rwNlV21YDno2If0valuxNsKv6ASuTjXhel7Q7sEuNfkdL6ifpQ2T3U86v0efXwMFphCRJq6YPAqy2HPEsj89K2kLSALJ7KBekEcxU4GOSdpLUFzgCeAW4vpNjPQFsnFtfLe3zDDAAOH454roMWE/SNyStLGk1Se9P284EjpM0DEDSEEl7L8exrYdwIrGWExEPkL0Rrko2+sj7KnCMpBeAH5K9kXb1uC+Q3QCeSnbj9zM1jv942vYo2Q3ngyPirhrHmkGW6E5L/e8BxnU1lhXwe7J7FY8Dq5CdBxExH/gs8EuyEcqeZDfSX+3kWCcA30+XnMaTJe4HyUYx84AbuxpU+p1+JD3v48A/gR3S5lPIfr9/S6/XjcD7ax3HejZFeGIrs1YmaTrwPxHxm7JjMavFIxIzMyvEicTMzArxpS0zMyvEIxIzMyukLYq4rb322tHR0VF2GGZmPcbMmTOfjoghXenbFomko6ODGTNmlB2GmVmPIenBZffK+NKWmZkV4kRiZmaFOJGYmVkhTiRmZlaIE4mZmRXiRGJmZoU4kZiZWSFOJGZmVkhbfCFx7oKFdHz3z2WHYWbWNA+c+LGmPZdHJGZmVogTiZmZFdJSiUTSYkmzJd0u6VJJg1N7h6SQdGyu79qSXpN0WnkRm5lZSyUSYFFEjIyILYFngUNy2+4D9sitfwq4o5nBmZnZ0lotkeTdAAzNrS8C7pQ0Kq3vB0xtelRmZraElkwkkvoAOwHTqjadB4yRtCGwGHi0k2McKGmGpBmLX17YuGDNzNpcqyWS/pJmA88AawJXVm2/HPgIMBb4Y2cHiohJETEqIkb1GTCoIcGamVnrJZJFETESGAb0Y8l7JETEq8BM4AjgwuaHZ2Zm1VotkQAQEQuBQ4HxkvpWbf458J2IeKb5kZmZWbWWTCQAETELmAOMqWq/IyLOLicqMzOr1lIlUiJiYNX6nrnVLWv0nwxMbmxUZmbWmZYdkZiZWc/QUiOSRnn30EHMaGIBMzOzduIRiZmZFeJEYmZmhbTFpS3PR2JmZWrm3CBl8IjEzMwKcSIxM7NCuj2RSHqxRtumkqanuUbulDRJ0q5pfbakFyXNT8vn5PY7RdICSW9L61/I7fOqpLlp+cTuPg8zM+uaZt0jORWYGBF/ApD07oiYC1yR1qcD4yNiRmWHlDz2BR4GtgemR8TvgN+l7Q8AO0TE0006BzMzq6FZl7bWBx6prKQksiw7ALcDZ5BV+zUzsxbUrEQyEbha0l8lHV6ZQncZxgJTgIuBPWoUb+yU5yMxM2uOpiSSdElqc+B8YDRwo6SV6/WX1A/4KHBJRDwP3ATsspzP6flIzMyaoGmf2oqIRyPirIjYG3idGkUYc3YDBgFz072Q7fDlLTOzltSURCJpt8qlKUnrAWsBCzrZZSzw5YjoiIgO4B3ALpIGNDxYMzNbLo341NYASY/k1n8BbAicIunfqe1bEfF4rZ1TstgVOKjSFhEvSfo7sCfLmGLXzMyaq9sTSUTUG+V8s5N9RueWXyabr726z8er1jtWLEIzM+tObVFry2XkzcwaxyVSzMysECcSMzMrpC0ubbmMfO8vY21m5fGIxMzMCnEiMTOzQlomkUhanErC3yFpjqRv5srHj5Z0WVpeV9Jlqc88SX8pN3Izs/bWSvdIFkXESABJ6wDnkpVJ+VFVv2OAKyPilNR3q6ZGaWZmS2iZEUleRDwJHAh8TZKqNleXpL+tmbGZmdmSWjKRAETEfWTxrVO16XTgt5KukfQ9SRvU2t9l5M3MmqNlE0lSPRohIq4ANgZ+DWwGzJI0pEY/l5E3M2uClk0kkjYGFgNPVm+LiGcj4tyI+BxwC9lUvGZmVoKWTCRphHEmcFpERNW2HSvl5CWtBgwHHmp+lGZmBq31qa3+kmYDfckmvvo9WQn6atsAp0l6nSwR/iYibmlemGZmltcyiSQi+nSybTowPS3/DPhZc6IyM7NlaZlE0kguI29m1jgteY/EzMx6DicSMzMrxInEzMwKaYt7JM2ej8Rzf5hZO/GIxMzMCnEiMTOzQpqeSCSFpJ/n1sdLmpBbP1DSXelxs6TtUnsfSTMlbZ/r+zdJn2rqCZiZ2RLKGJG8Anxc0trVGyTtARwEbBcRmwEHA+dKWi8iFgNfBU6X1FfSWCAi4vxmBm9mZksqI5G8DkwCDq+x7TvAtyLiaYCIuBU4Gzgkrd8EXA9MAI6vtJuZWXnKukdyOrC/pOr67u8CZla1zUjtFUcC3wDOjYh76j2B5yMxM2uOUhJJRDwPnAMc2oXuAvIVgLcHFgJbLuM5PB+JmVkTlPmprZOBLwGr5trmkVX3zds6tSNpVeCnwI7AEEkfbUKcZmbWidISSUQ8C0wlSyYVPwV+ImktAEkjgXHAr9L2HwJTI+IushvvEyWt0rSgzcxsKWV/s/3nwNcqKxExTdJQ4HpJAbwAfDYiHpO0BbAv8J7Ud7akK8hu0B/d/NDNzAxKSCQRMTC3/AQwoGr7GcAZNfabB2xS1daVeyxmZtZAZY9ImsLzkZiZNY5LpJiZWSFOJGZmVkhbXNpqVhl5l483s3bkEYmZmRXiRGJmZoUsM5FIWixptqTbJZ0vaWhany3pcUkLcuv9qvpfKmlw1fEOl/TvSp0tSbvm9n9R0vy0fI6k0ZIuy+27j6TbUon5uZL26f5fiZmZLY+ujEgWRcTIiNgSeBXYL62PBM4EJlbWI+LVqv7PsnSF3rHALWRfLiQirsgdbwawf1r/fH4nSe8BTgL2TiXm9wJOkrTVCp+9mZkVtryXtq4D3rkc/W8AhlZWJA0HBgLfJ0soy2M8cHxE3A+Qfp4AfGs5j2NmZt2oy4lE0krA7sDcLvbvA+wETMs1jwWmkCWkTSWt0/VQu1RiPv/8LiNvZtYEXUkk/SXNJnvTfgj4bRf7PwOsCVyZ2zYGOC8i3gAuApZnmtzqcvL12gCXkTcza5aufI9kUbp/0VWLImJkupl+Gdk9klPTvYwRwJWSAPoB95FNctUVdwCjgNtybW+WmDczs3I07OO/EbGQbOKq8ZL6kl3WmhARHemxATBU0rAuHvIk4EhJHQDp51FkFYTNzKwkDf0eSUTMAuaQXdIaA1xc1eXi1N6VY80mKxl/qaS7gEuBb6d2MzMriSJq3mLoVVZef0Ssf8DJDX8el0gxs95C0syIGNWVvm1Ra8tl5M3MGsclUszMrBAnEjMzK6QtLm11Vxl53wMxM1uaRyRmZlaIE4mZmRXStEQiaT1J50m6V9I8SX+RtImkRals/LxUOr5v6v9mCXlJ4ySFpJ1yx9s3tX2yWedgZmZLa0oiUVYT5WJgekQMj4gtyL6Vvi5wbyrB8m5gQ+DTdQ4zlyUrBo8h+7KjmZmVqFkjkh2A1yLizEpD+kb6w7n1xcDN5MrOV7kO2FZSX0kDycrZ+1vtZmYla1Yi2ZKlS8AvQdIqwPuBy+t0CeB/gV2BvVmyPL2ZmZWkFW62D8+VnX8oIm7rpO95vFW3a0pnB/V8JGZmzdGsRHIHsE2dbZV7JO8EPiBpr3oHiYibyUY3a0fE3Z09oecjMTNrjmYlkquBlSV9pdIg6X3AmyXkI+Ix4LvAkcs41pFkN+rNzKwFNCWRRFZieF/gI+njv3cAE4BHq7peAgyQ9KFOjvXXiLimYcGamdlyaVqJlIh4lNof7d0y1yeA9+S2TU/tk4HJNY45rhtDNDOzFdAKN9vNzKwHa4uijZ6PxMyscTwiMTOzQpxIzMyskLa4tNWV+Ug814iZ2YrxiMTMzApxIjEzs0J6XCKRtDjNXzJH0q2S/rPsmMzM2llPvEeyKNXmQtKuwAnAh8sNycysffW4EUmV1YF/lR2EmVk764kjkv6p7PwqwPrAjrU6SToQOBCgz+pDmhedmVmb6YkjkkURMTIiNgN2A85JU/kuwWXkzcyaoycmkjdFxA3A2oCHHGZmJenRiUTSZkAfstkVzcysBD35HgmAgAMiYnGZAZmZtbMel0giok/ZMZiZ2Vt6XCJZES4jb2bWOD36HomZmZXPicTMzAppi0tbyyoj7xLyZmYrziMSMzMrxInEzMwKKS2RSForlYOfLelxSQty6/0k7Ssp0pcOK/uMknS7pH5pfbik+yStXtZ5mJm1u9ISSUQ8k2pmjQTOBCZW1iPiVWAs8HdgTG6fGcC1wPjUdDrwvYh4vsnhm5lZ0pI32yUNBD4I7ABMAybkNh8F3CrpdaBvRExpfoRmZlbRkokE2Ae4PCLulvSspK0j4laAiHhO0k+AXwFblBqlmZm17M32scB5afm8tJ63O/AEnSQSSQdKmiFpxuKXFzYmSjMza70RiaS1yCar2lJSkFX3DUnfjoiQtAcwCNgVuFjSFRHxcvVxImISMAlg5fVHRPPOwMysvbTiiOSTwDkRMSwiOiJiI+B+YDtJ/YGfA4dExFzgT8D3SozVzKzttWIiGQtcXNV2IfAZ4AfAJRExL7VPAMZIGtG88MzMLK8lLm1FxITc8uga20+ts98LwPCGBWZmZsvUiiMSMzPrQVpiRNJono/EzKxxPCIxM7NCnEjMzKyQtri0VW8+Es9DYmZWnEckZmZWiBOJmZkVUmoiqTPnyAhJl0m6V9JMSddI2j5tGyfpqdy8JbMluXCjmVmJyh6RLDHniKRVgD8DkyJieERsA3wd2Di3zx9z85aMzH3L3czMSlDmDImVOUe+xFuTV+0P3BAR0yr9IuL2iJjc/AjNzKwryvzU1lJzjgDvAm5dxn77Sdout/4fEbGoupOkA4EDAfqsPqS7YjYzsyplXtpa1pwjSLo4zdF+Ua65+tLWUkkEsjLyETEqIkb1GTCo+6M3MzOgpBFJvTlHgKOB7Sv9ImJfSaOAk8qI08zMlq2sEUm9OUfuBj4oaa9c3wGlRGhmZl1S1j2SscCJVW2VOUf2AH4h6WSy6XRfAH6c61d9j+SrEXF9I4M1M7P6SkkkXZhz5KN19psMTG5IUGZmtkLaotaWy8ibmTVO2V9INDOzHs6JxMzMCmmLRDJ3wcKyQzAz67XaIpGYmVnjOJGYmVkh3ZpIJL2Yfnak8vBfz207TdK4tDxZ0v2S5ki6W9I5koZWHye3Pk7SaWl5U0nTUwn5OyVN6s5zMDOz5dPIEcmTwGGS+tXZ/q2IeA+wKTALuKaTvnmnAhNTna3NgV92T7hmZrYiGplIngKuAg7orFNkJgKPA7t34bjrA4/k9p9bJEgzMyum0fdITgSOkNSnC31vBTZbZi+YCFwt6a+SDpc0uFYnSQdKmiFpxuKX/aktM7NGaWgiiYj7gZvJamgti5Z1uHTM3wGbA+cDo4EbJa1c47ldRt7MrAma8amt44HvdOG53gvcmZYXVd0vWRN4urISEY9GxFkRsTfwOrBlN8ZrZmbLoeGJJCLuAuaRVfVdijKHkt37uDw1/x/w2bS9P/Bp4Jq0vpukvml5PWAtYEEjz8HMzOpr1vdIjgM2rGr7maQ5ZHOQvA/YISJeTdsOAz4uaTZwI3B+RFybtu0C3J72vYLs01+PN/wMzMysJkVE2TE03Mrrj4hXHvtn2WGYmfUYkmZGxKiu9PU3283MrJC2SCTvHupPbZmZNUpbJBIzM2scJxIzMyukLRKJ5yMxM2uctkgkZmbWOE4kZmZWSCmJRNLiNJ/I7ZIurS68mIox/lvSoFzbaEkLJc2SNF/StZJqflvezMyap6wRyaI0n8iWwLPAIVXbxwK3APtWtV8XEe+NiE2BQ4HTJO3U+HDNzKyeVri0dQOQnx1xODAQ+D5ZQqkpImYDxwBfa3SAZmZWX6mJJM1TshMwLdc8FpgCXAdsKmmdTg5Rdw4Tz0diZtYcZSWS/qkg4zNkJeKvzG0bA5wXEW8AFwGf6uQ4decw8XwkZmbNUeo9EmAY0I90j0TSVsAI4EpJD5AllbqXt1hyDhMzMytBqZe2ImIh2U3z8WmOkbHAhIjoSI8NgKGShlXvm5LOD4DTmxq0mZktYaWyA4iIWWlukTHpsXtVl4tT+03AhyTNAgYATwKHRsRVzYzXzMyW5PlIzMxsKZ6PpIrLyJuZNU5bJBIzM2scJxIzMyvEicTMzApxIjEzs0KcSMzMrJCGJxJJ60k6T9K9kuZJ+oukTSTdXtVvgqTxufWVJD0t6YSqfnukUvJz0vEOavQ5mJlZfQ39QqIkkX2h8OyIGJPaRgLrdmH3XYD5wKclHRURkb79PgnYNiIekbQy0NGY6M3MrCsaPSLZAXgtIs6sNKTy7w93Yd+xwCnAQ8AHUttqZMnvmXSsVyJifrdGbGZmy6XRiWRLYGadbcPTLImzUyXggysbJPUnKy9/GVlJ+bEAEfEsWcn5ByVNkbS/pJrnkC8j/9RTT3XjKZmZWV6ZN9vvTbMkjkyVgM/MbdsDuCYiXgYuBPZNc5cQEV8mSzI3A+OBs2odPF9GfsiQIQ09ETOzdtboRHIHsM0K7DcW2DmVkp8JrEV2mQyAiJgbEROBjwCf6IY4zcxsBTU6kVwNrCzpK5UGSe8jm4ekJkmrA9sBb6+Ukyebr2SspIGSRue6jwQebETgZmbWNQ1NJJGVFt4X+Ej6+O8dwATg0U52+zhwdUS8kmv7E7AX0Af4tqT56b7K0cC4RsRuZmZd0xZl5EeNGhUzZswoOwwzsx7DZeTNzKxpnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK6TlEomkffPFHNPjDUn/JSkkfT3X9zRJ40oM18ys7bVcIomIi6uKOf4KuA64AngSOExSv1KDNDOzN7VcIsmTtAnwQ+BzwBvAU8BVwAFlxmVmZm9p2USSZkM8FxgfEQ/lNp0IHFEpK9/J/p6PxMysCVo2kQDHAndExHn5xoi4n2wuks90trPnIzEza46Gztm+olKp+E8AW9fpcjxwAXBts2IyM7PaWm5EImkN4HfA5yPihVp9IuIuYB7ZTIpmZlaiVhyRHAysA5whKd8+parfccCsZgVlZma1tVwiiYgTgBPqbP5Jrt8cWnBEZWbWbvxGbGZmhTiRmJlZIU4kZmZWiBOJmZkV4kRiZmaFOJGYmVkhTiRmZlaIE4mZmRXiRGJmZoU4kZiZWSGKiLJjaDhJLwDzy46jJGsDT5cdRIl8/j7/dj3/ouc+LCK6NAdHy9XaapD5ETGq7CDKIGlGu547+Px9/u17/s08d1/aMjOzQpxIzMyskHZJJJPKDqBE7Xzu4PP3+bevpp17W9xsNzOzxmmXEYmZmTWIE4mZmRXSqxOJpN0kzZd0j6Tvlh1Po0naSNI1ku6UdIekw1L7mpKulPTP9HONsmNtFEl9JM2SdFlaf4ekm9K5/1FSv7JjbBRJgyVdIOmu9G/gP9rstT88/bu/XdIUSav05tdf0lmSnpR0e66t5uutzKnpvfA2SVt3Zyy9NpFI6gOcDuwObAGMlbRFuVE13OvAERGxOfAB4JB0zt8FroqIEcBVab23Ogy4M7f+E2BiOvd/AV8qJarmOAW4PCI2A95D9ntoi9de0lDgUGBURGwJ9AHG0Ltf/8nAblVt9V7v3YER6XEgcEZ3BtJrEwmwLXBPRNwXEa8C5wF7lxxTQ0XEYxFxa1p+geyNZCjZeZ+dup0N7FNOhI0laUPgY8Bv0rqAHYELUpfefO6rA9sDvwWIiFcj4jna5LVPVgL6S1oJGAA8Ri9+/SPiWuDZquZ6r/fewDmRuREYLGn97oqlNyeSocDDufVHUltbkNQBvBe4CVg3Ih6DLNkA65QXWUOdDHwbeCOtrwU8FxGvp/Xe/G9gY+Ap4Hfp0t5vJK1Km7z2EbEAOAl4iCyBLARm0j6vf0W917uh74e9OZGoRltbfNZZ0kDgQuAbEfF82fE0g6Q9gCcjYma+uUbX3vpvYCVga+CMiHgv8BK99DJWLelewN7AO4ANgFXJLudU662v/7I09P9Cb04kjwAb5dY3BB4tKZamkdSXLIn8ISIuSs1PVIax6eeTZcXXQB8E9pL0ANllzB3JRiiD06UO6N3/Bh4BHomIm9L6BWSJpR1ee4Cdgfsj4qmIeA24CPhP2uf1r6j3ejf0/bA3J5JbgBHpUxv9yG68TSs5poZK9wR+C9wZEb/IbZoGHJCWDwD+1OzYGi0ijoyIDSOig+y1vjoi9geuAT6ZuvXKcweIiMeBhyVtmpp2AubRBq998hDwAUkD0v+Dyvm3xeufU+/1ngZ8Pn166wPAwsolsO7Qq7/ZLumjZH+V9gHOiojjSg6poSRtB1wHzOWt+wRHkd0nmQq8new/3KciovomXa8haTQwPiL2kLQx2QhlTWAW8NmIeKXM+BpF0kiyDxr0A+4DvkD2x2JbvPaSjgb2I/v04izgy2T3AXrl6y9pCjCarFz8E8CPgEuo8Xqn5Hoa2ae8Xga+EBEzui2W3pxIzMys8XrzpS0zM2sCJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEuuxJC2WNDtVe71U0uAu7PPiMrYPlvTV3PoGki7obJ8uxtqRr9LaDJJGpo/AmzWUE4n1ZIsiYmSq9voscEg3HHMw8GYiiYhHI+KTnfRvSenb3CMBJxJrOCcS6y1uIFeETtK3JN2S5l44urqzpIGSrpJ0q6S5kiqVoU8EhqeRzs/yI4k0r8W7cseYLmkbSaumuSFuSQUTO60yLWmcpEvSKOp+SV+T9M20742S1swd/2RJ16dR17apfc20/22p/1apfYKkSZL+BpwDHAPsl85lP0nbpmPNSj83zcVzkaTLlc1j8dNcrLul39EcSVeltuU6X2sDEeGHHz3yAbyYfvYBzgd2S+u7AJPICtW9DbgM2L5qn5WA1dPy2sA9qX8HcHvuOd5cBw4Hjk7L6wN3p+Xjyb4xDdmI5m5g1apY88cZl55vNWAIWaXag9O2iWTFNgGmA79Oy9vn9v8l8KO0vCMwOy1PIKt42z/3PKflYlgdWCkt7wxcmOt3HzAIWAV4kKwu0xCyirHvSP3W7Or5+tFej0oxM7OeqL+k2WRv0jOBK1P7LukxK60PJJvQ59rcvgKOl7Q9WTmZocC6y3i+qek5fgR8mix5VZ5vL0nj0/oqZCUq7lzqCG+5JrI5Y16QtBC4NLXPBbbK9ZsC2dwTklZP94G2Az6R2q+WtJakQan/tIhYVOc5BwFnSxpBVvm1b27bVRGxEEDSPGAYsAZwbUTcn56rUlplRc7XejEnEuvJFkXEyPQmehnZPZJTyZLECRHx353suz/ZX9zbRMRrqWrwKp09WUQskPRMupS0H3BQ2iTgExExfzliz9d7eiO3/gZL/r+srmEUdF4S/KVOnvNYsgS2r7L5aqbXiWdxikE1nh9W7HytF/M9Euvx0l/ShwLjlZXRvwL4orJ5WZA0VFL1hE6DyOYveU3SDmR/gQO8QHbJqZ7zyCbPGhQRc1PbFcDXU2E8JL23O85n9JICAAAA7klEQVQr2S8dczuyiq0LyUZW+6f20cDTUXvemepzGQQsSMvjuvDcNwAflvSO9FxrpvZGnq/1QE4k1itExCxgDjAmIv4GnAvcIGku2dwc1cnhD8AoSTPI3pTvSsd5BvhHurn9sxpPdQFZmfqpubZjyS4T3ZZuzB/bfWfGvyRdD5zJW/ONT0ix30b24YAD6ux7DbBF5WY78FPgBEn/ILuv1KmIeIpsfu+LJM0B/pg2NfJ8rQdy9V+zFiVpOlk5/G4r923WCB6RmJlZIR6RmJlZIR6RmJlZIU4kZmZWiBOJmZkV4kRiZmaFOJGYmVkh/w/8hknZJwNZFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimador_gradientboost.fit(boston[datos.feature_names], boston.objetivo)\n",
    "\n",
    "importancia_variables = estimador_gradientboost.feature_importances_\n",
    "importancia_variables = 100.0 * (importancia_variables / importancia_variables.max())\n",
    "sorted_idx = np.argsort(importancia_variables)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.barh(pos, importancia_variables[sorted_idx], align='center')\n",
    "plt.yticks(pos, datos.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosques Aleatorios (Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Bosques Aleatorios funciona mediante la creación de árboles de decision entrenados en un subgrupo aleatorio de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest regressor.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of classifying\n",
      "    decision trees on various sub-samples of the dataset and uses averaging\n",
      "    to improve the predictive accuracy and control over-fitting.\n",
      "    The sub-sample size is always the same as the original\n",
      "    input sample size but the samples are drawn with replacement if\n",
      "    `bootstrap=True` (default).\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "        .. versionchanged:: 0.20\n",
      "           The default value of ``n_estimators`` will change from 10 in\n",
      "           version 0.20 to 100 in version 0.22.\n",
      "\n",
      "    criterion : string, optional (default=\"mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"mse\" for the mean squared error, which is equal to variance\n",
      "        reduction as feature selection criterion, and \"mae\" for the mean\n",
      "        absolute error.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    min_impurity_split : float, (default=1e-7)\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19. The default value of\n",
      "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether bootstrap samples are used when building trees. If False, the\n",
      "        whole datset is used to build each tree.\n",
      "\n",
      "    oob_score : bool, optional (default=False)\n",
      "        whether to use out-of-bag samples to estimate\n",
      "        the R^2 on unseen data.\n",
      "\n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "\n",
      "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
      "    ...                              n_estimators=100)\n",
      "    >>> regr.fit(X, y)\n",
      "    RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "               max_features='auto', max_leaf_nodes=None,\n",
      "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "               min_samples_leaf=1, min_samples_split=2,\n",
      "               min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "               oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "    >>> print(regr.feature_importances_)\n",
      "    [0.18146984 0.81473937 0.00145312 0.00233767]\n",
      "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "    [-8.32987858]\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    The default value ``max_features=\"auto\"`` uses ``n_features`` \n",
      "    rather than ``n_features / 3``. The latter was originally suggested in\n",
      "    [1], whereas the former was more recently justified empirically in [2].\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized \n",
      "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, ExtraTreesRegressor\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(RandomForestRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de scikit-learn de RandomForest hace que cada árbol se entrene en base a un dataset del mismo tamaño que el original (con reemplazo si se usa la opción `bootstrap=True`).\n",
    "\n",
    "En cuanto al criterio para evaluar la calidad de la separación de un node de cada árbol base, para la implementación de Regresion, `RandomForestRegressor` usa el error medio cuadrático `mse` por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.222490728063017"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_randomforest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_randomforest, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"randomforest_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.565069432592691,\n",
       " 'elasticnet': 5.261057069533587,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969341,\n",
       " 'bagging_arbol_10': 4.324085210844261,\n",
       " 'bagging_arbol_100': 4.139356094305253,\n",
       " 'bagging_elnet': 5.272961905392164,\n",
       " 'bagging_extra_arbol': 3.8602984269332885,\n",
       " 'adaboost_100': 4.432473271209594,\n",
       " 'gradientboost_100': 3.9452443022624912,\n",
       " 'randomforest_100': 4.222490728063017}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) es un algoritmo de boosting relativamente nuevo que tiene bastante acogida. Es una implementación de Gradient Boosted Trees pero enfocado a datasets grandes.\n",
    "\n",
    "Al ser muy nuevo (el proyecto se creó en 2014 y el paper se publicó en 2016, [éste es el paper](https://arxiv.org/abs/1603.02754)) no está implementado en scikit-learn, sin embargo existe en el paquete [xgboost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html), que proporciona estimadores en base a dicho algoritmo que son compatibles con sklearn.\n",
    "\n",
    "Podemos instalar `xgboost` de conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    n_estimators : int\n",
      "        Number of boosted trees to fit.\n",
      "    silent : boolean\n",
      "        Whether to print messages while running boosting.\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: string\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    nthread : int\n",
      "        Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "    min_child_weight : int\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : int\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each split, in each level.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    seed : int\n",
      "        Random number seed.  (Deprecated, please use random_state)\n",
      "    random_state : int\n",
      "        Random number seed.  (replaces seed)\n",
      "    missing : float, optional\n",
      "        Value in the data which needs to be present as a missing value. If\n",
      "        None, defaults to np.nan.\n",
      "    importance_type: string, default \"gain\"\n",
      "        The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "        \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "    \\*\\*kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "        be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "        will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "            passed via this argument will interact properly with scikit-learn.\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "    A custom objective function can be provided for the ``objective``\n",
      "    parameter. In this case, it should have the signature\n",
      "    ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "    y_true: array_like of shape [n_samples]\n",
      "        The target values\n",
      "    y_pred: array_like of shape [n_samples]\n",
      "        The predicted values\n",
      "\n",
      "    grad: array_like of shape [n_samples]\n",
      "        The value of the gradient for each sample point.\n",
      "    hess: array_like of shape [n_samples]\n",
      "        The value of the second derivative for each sample point\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(XGBRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.019934086350415"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_xgboost = XGBRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_xgboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"xgboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import plot_importance, to_graphviz\n",
    "estimador_xgboost.fit(boston[datos.feature_names], boston.objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos un modelo entrenado, podemos ver la importancia de las variables en partir los árboles mediante el método `plot_importance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x287e0173da0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(estimador_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `to_graphviz` imprime un árbol en concreto (pasandole el parámetro `rankdir='LR'` lo imprime en horizontal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"992pt\" height=\"422pt\"\r\n",
       " viewBox=\"0.00 0.00 992.44 422.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 418)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-418 988.441,-418 988.441,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"67.594\" cy=\"-198\" rx=\"67.6881\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"67.594\" y=\"-194.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RM&lt;6.5454998</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"327.28\" cy=\"-234\" rx=\"87.1846\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"327.28\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTAT&lt;19.6450005</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M127.712,-206.259C162.358,-211.099 206.629,-217.284 244.574,-222.585\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"244.195,-226.066 254.583,-223.984 245.164,-219.134 244.195,-226.066\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"187.688\" y=\"-222.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"327.28\" cy=\"-153\" rx=\"81.7856\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"327.28\" y=\"-149.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">NOX&lt;0.659000039</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M124.508,-188.236C162.114,-181.669 212.078,-172.943 252.878,-165.818\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"253.522,-169.259 262.771,-164.091 252.318,-162.363 253.522,-169.259\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"187.688\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"616.864\" cy=\"-342\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"616.864\" y=\"-338.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;1.37275004</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M370.097,-249.704C422.121,-269.241 510.835,-302.557 566.245,-323.366\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"565.259,-326.734 575.851,-326.973 567.72,-320.181 565.259,-326.734\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"466.873\" y=\"-300.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"616.864\" cy=\"-234\" rx=\"97.4827\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"616.864\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">PTRATIO&lt;19.6500015</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M414.565,-234C444.348,-234 478.073,-234 509.286,-234\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"509.323,-237.5 519.323,-234 509.323,-230.5 509.323,-237.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"466.873\" y=\"-237.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>5</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"616.864\" cy=\"-153\" rx=\"72.2875\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"616.864\" y=\"-149.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RM&lt;7.43700027</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;5 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>2&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M409.537,-153C448.57,-153 495.238,-153 534.309,-153\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"534.591,-156.5 544.591,-153 534.591,-149.5 534.591,-156.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"466.873\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>6</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"616.864\" cy=\"-72\" rx=\"87.1846\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"616.864\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTAT&lt;24.5300007</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M378.374,-138.891C427.357,-125.094 502.035,-104.061 554.545,-89.271\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"555.732,-92.5729 564.408,-86.4929 553.834,-85.8351 555.732,-92.5729\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"466.873\" y=\"-125.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-396\" rx=\"70.6878\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-392.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=1.62423265</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M675.14,-352.928C721.286,-361.732 786.221,-374.121 834.611,-383.353\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"834.194,-386.837 844.673,-385.273 835.506,-379.961 834.194,-386.837\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-379.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-342\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-338.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.601945102</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;8 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M690.018,-342C728.519,-342 776.121,-342 816.349,-342\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"816.408,-345.5 826.408,-342 816.408,-338.5 816.408,-345.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-345.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-288\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-284.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.537518978</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;9 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>4&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M685.298,-246.866C729.837,-255.363 788.01,-266.462 832.704,-274.989\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"832.122,-278.441 842.601,-276.878 833.434,-271.565 832.122,-278.441\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-271.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-234\" rx=\"70.6878\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.22515808</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;10 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M714.477,-234C749.017,-234 787.539,-234 820.559,-234\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"820.619,-237.5 830.619,-234 820.619,-230.5 820.619,-237.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-237.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-180\" rx=\"70.6878\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-176.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=1.00345886</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>5&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M684.581,-159.364C726.731,-163.385 781.306,-168.592 825.093,-172.769\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"824.831,-176.259 835.118,-173.725 825.495,-169.291 824.831,-176.259\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-173.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-126\" rx=\"70.6878\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=1.51664674</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>5&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M684.581,-146.636C726.731,-142.615 781.306,-137.408 825.093,-133.231\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"825.495,-136.709 835.118,-132.275 824.831,-129.741 825.495,-136.709\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-72\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.383107781</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>6&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M704.321,-72C739.832,-72 780.823,-72 816.155,-72\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"816.442,-75.5001 826.442,-72 816.442,-68.5001 816.442,-75.5001\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-75.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"901.898\" cy=\"-18\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"901.898\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0196402315</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>6&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M681.382,-59.8813C725.173,-51.5266 783.454,-40.4072 828.958,-31.7254\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"829.885,-35.1118 839.051,-29.7997 828.573,-28.2359 829.885,-35.1118\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"766.855\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x287e02ab8d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_graphviz(estimador_xgboost, num_trees=11, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el output de un árbol no está en la misma escala que las predicciones (la variable objetivo tiene el rango 5-50), esto es así por que en el algoritmo XGBoost cada árbol se basa en el output del árbol anterior, intentando corregir el error producido por el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de stacking simplemente usa el output (generalmente en terminos de probabilidades para casos de clasificacion o de las predicciones en casos de regresión) de múltiples modelos como input para un nuevo *metamodelo*.\n",
    "\n",
    "scikit learn no tiene un estimador de stacking por defecto, sin embargo, podemos usar el  estimador de stacking (`StackingRegressor`) de [mlxtend](https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/), una librería que amplia las funcionalidades de `sklearn`\n",
    "\n",
    "Podemos instalar mlxtend asi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Stacking regressor for scikit-learn estimators for regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    regressors : array-like, shape = [n_regressors]\n",
      "        A list of regressors.\n",
      "        Invoking the `fit` method on the `StackingRegressor` will fit clones\n",
      "        of those original regressors that will\n",
      "        be stored in the class attribute\n",
      "        `self.regr_`.\n",
      "    meta_regressor : object\n",
      "        The meta-regressor to be fitted on the ensemble of\n",
      "        regressors\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "        - `verbose=0` (default): Prints nothing\n",
      "        - `verbose=1`: Prints the number & name of the regressor being fitted\n",
      "        - `verbose=2`: Prints info about the parameters of the\n",
      "                       regressor being fitted\n",
      "        - `verbose>2`: Changes `verbose` param of the underlying regressor to\n",
      "           self.verbose - 2\n",
      "    use_features_in_secondary : bool (default: False)\n",
      "        If True, the meta-regressor will be trained both on\n",
      "        the predictions of the original regressors and the\n",
      "        original dataset.\n",
      "        If False, the meta-regressor will be trained only on\n",
      "        the predictions of the original regressors.\n",
      "    store_train_meta_features : bool (default: False)\n",
      "        If True, the meta-features computed from the training data\n",
      "        used for fitting the\n",
      "        meta-regressor stored in the `self.train_meta_features_` array,\n",
      "        which can be\n",
      "        accessed after calling `fit`.\n",
      "\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    regr_ : list, shape=[n_regressors]\n",
      "        Fitted regressors (clones of the original regressors)\n",
      "    meta_regr_ : estimator\n",
      "        Fitted meta-regressor (clone of the original meta-estimator)\n",
      "    coef_ : array-like, shape = [n_features]\n",
      "        Model coefficients of the fitted meta-estimator\n",
      "    intercept_ : float\n",
      "        Intercept of the fitted meta-estimator\n",
      "    train_meta_features : numpy array, shape = [n_samples, len(self.regressors)]\n",
      "        meta-features for training data, where n_samples is the\n",
      "        number of samples\n",
      "        in training data and len(self.regressors) is the number of regressors.\n",
      "    refit : bool (default: True)\n",
      "        Clones the regressors for stacking regression if True (default)\n",
      "        or else uses the original ones, which will be refitted on the dataset\n",
      "        upon calling the `fit` method. Setting refit=False is\n",
      "        recommended if you are working with estimators that are supporting\n",
      "        the scikit-learn fit/predict API interface but are not compatible\n",
      "        to scikit-learn's `clone` function.\n",
      "\n",
      "    Examples\n",
      "    -----------\n",
      "    For usage examples, please see\n",
      "    http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(StackingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos usar los estimadores ensamblados que hemos creado en este notebook para crear un nuevo estimador. Dicho estimador no tiene garantizado un funcionamiento mejor que el mejor de los estimadores que usa como input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.047173506657087"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_stacking = StackingRegressor(\n",
    "    regressors=[\n",
    "        BaggingRegressor(n_estimators=100),\n",
    "        AdaBoostRegressor(n_estimators=100),\n",
    "        GradientBoostingRegressor(n_estimators=100),\n",
    "        RandomForestRegressor(n_estimators=100)\n",
    "    ], \n",
    "    meta_regressor=XGBRegressor(n_estimators=100))\n",
    "\n",
    "\n",
    "error_cv = cross_val_score(estimador_stacking, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"stacking\"] = error_cv\n",
    "\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.565069432592691,\n",
       " 'elasticnet': 5.261057069533587,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969341,\n",
       " 'bagging_arbol_10': 4.324085210844261,\n",
       " 'bagging_arbol_100': 4.139356094305253,\n",
       " 'bagging_elnet': 5.272961905392164,\n",
       " 'bagging_extra_arbol': 3.8602984269332885,\n",
       " 'adaboost_100': 4.432473271209594,\n",
       " 'gradientboost_100': 3.9452443022624912,\n",
       " 'randomforest_100': 4.222490728063017,\n",
       " 'xgboost_100': 4.019934086350415,\n",
       " 'stacking': 4.047173506657087}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
