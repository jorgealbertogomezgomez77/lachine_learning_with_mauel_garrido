{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Clustering. Detección de Anomalías\n",
    "\n",
    "Uno de los usos de los algoritmos de clustering es la Detección de Anomalías, esto es, la detección de observaciones anómalas, aquellas que no siguen un comportamiento normal. Si el objetivo del clustering es encontrar grupos de elementos similares, aquellos elementos que no son similares a ningún grupo se pueden considerar como elementos anómalos.\n",
    "\n",
    "Para este ejercicio vamos a usar un [Dataset de transacciones de tarjetas de crédito](https://www.kaggle.com/arjunbhasin2013/ccdata), donde cada observacion es un cliente distinto.\n",
    "\n",
    "Nuestro objetivo es implementar un modelo que agrupa las transacciones apropiadamente y encontrar los potenciales outliers, es decir, aquellas transacciones que son sospechosas de ser un fraude o un error. Para resolver este ejercicio correctamente hay que investigar, en vez de simplemente seguir a rajatabla lo enseñado en el curso.\n",
    "\n",
    "**Pistas:**\n",
    "\n",
    "- Hemos explicado un algoritmo de clustering que no solo asigna elementos a clusters válidos, sino que también clasifica elementos como valores extremos (outliers). \n",
    "\n",
    "- Para la búsqueda de hiperparámetros, un buen sitio para mirar es [ParameterSampler](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CC GENERAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8950, 18)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUST_ID                              object\n",
       "BALANCE                             float64\n",
       "BALANCE_FREQUENCY                   float64\n",
       "PURCHASES                           float64\n",
       "ONEOFF_PURCHASES                    float64\n",
       "INSTALLMENTS_PURCHASES              float64\n",
       "CASH_ADVANCE                        float64\n",
       "PURCHASES_FREQUENCY                 float64\n",
       "ONEOFF_PURCHASES_FREQUENCY          float64\n",
       "PURCHASES_INSTALLMENTS_FREQUENCY    float64\n",
       "CASH_ADVANCE_FREQUENCY              float64\n",
       "CASH_ADVANCE_TRX                      int64\n",
       "PURCHASES_TRX                         int64\n",
       "CREDIT_LIMIT                        float64\n",
       "PAYMENTS                            float64\n",
       "MINIMUM_PAYMENTS                    float64\n",
       "PRC_FULL_PAYMENT                    float64\n",
       "TENURE                                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = df.CUST_ID\n",
    "df = df.drop(columns=\"CUST_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CREDIT_LIMIT', 'MINIMUM_PAYMENTS'], dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 17)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputamos a 0 los valores nulos, así podemos ver si los elementos anómalos son aquellos que tienen estas columnas a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos aprendido el algoritmo DBSCAN, lo bueno de usar este algoritmo para detección de anomalías es que no asigna un cluster a todos los puntos, sino aquellos puntos que están más separados del resto se etiquetan automáticamente como valores extremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar un algoritmo de clustering basado en densidad tenemos que estandarizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalizado = pd.DataFrame(StandardScaler().fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1     6627\n",
       " 0     1948\n",
       " 10      60\n",
       " 2       34\n",
       " 15      30\n",
       " 7       23\n",
       " 14      23\n",
       " 8       14\n",
       " 6       13\n",
       " 3       11\n",
       " 29      10\n",
       " 21       9\n",
       " 5        9\n",
       " 9        8\n",
       " 12       8\n",
       " 1        8\n",
       " 26       8\n",
       " 17       7\n",
       " 11       7\n",
       " 27       7\n",
       " 19       7\n",
       " 4        6\n",
       " 13       6\n",
       " 23       6\n",
       " 28       5\n",
       " 30       5\n",
       " 16       5\n",
       " 24       5\n",
       " 22       5\n",
       " 25       5\n",
       " 20       5\n",
       " 18       5\n",
       " 35       5\n",
       " 31       5\n",
       " 32       4\n",
       " 34       4\n",
       " 33       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = DBSCAN()\n",
    "cluster_labels = clusterer.fit_predict(df_normalizado)\n",
    "pd.Series(cluster_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que por defecto DBSCAN produce demasiados outliers (más de 7000 elementos anómalos del total de 9000 transacciones. Podemos usar el coeficiente de silueta (`silhouette_score`) para ver como separa el algoritmo a los buenos clientes de los (potencialmente) malos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.46596190778573116"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(df_normalizado, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un coeficiente de silueta negativo es bastante malo. Podemos hacer una búsqueda aleatoria para optimizar dicho resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'eps': 0.5,\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'euclidean',\n",
       " 'metric_params': None,\n",
       " 'min_samples': 5,\n",
       " 'n_jobs': 1,\n",
       " 'p': None}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBSCAN().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform \n",
    "\n",
    "\n",
    "distribucion_parametros = {\n",
    "    \"eps\": uniform(0,5),\n",
    "    \"min_samples\": sp_randint(2, 20),\n",
    "    \"p\": sp_randint(1, 3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eps': <scipy.stats._distn_infrastructure.rv_frozen at 0x21816e979b0>,\n",
       " 'min_samples': <scipy.stats._distn_infrastructure.rv_frozen at 0x2181586efd0>,\n",
       " 'p': <scipy.stats._distn_infrastructure.rv_frozen at 0x218155fbf98>}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribucion_parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema que hay con el algoritmo HDBSCAN (o DBSCAN) es que no tiene el método `predict`, por lo tanto no podemos usar el método de busqueda aleatoria (RandomSearchCV) de scikit-learn.\n",
    "\n",
    "Sin embargo, podemos desarrollar nuestro propio método de búsqueda con  [`ParameterSampler`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html), que es lo que usa scikit-learn para tomar muestras del diccionario de búsqueda de hiperparámetros.\n",
    "\n",
    "**Este paso tarda tiempo en ejecutarse!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "n_muestras = 30 # probamos 20 combinaciones de hiperparámetros\n",
    "n_iteraciones = 3 #para validar, vamos a entrenar para cada selección de hiperparámetros en 3 muestras distintas\n",
    "pct_muestra = 0.7 # usamos el 70% de los datos para entrenar el modelo en cada iteracion\n",
    "resultados_busqueda = []\n",
    "lista_parametros = list(ParameterSampler(distribucion_parametros, n_iter=n_muestras))\n",
    "\n",
    "for param in lista_parametros:\n",
    "    for iteration in range(n_iteraciones):\n",
    "        param_resultados = []\n",
    "        muestra = df_normalizado.sample(frac=pct_muestra)\n",
    "        etiquetas_clusters = DBSCAN(n_jobs=-1, **param).fit_predict(muestra)\n",
    "        try:\n",
    "            param_resultados.append(silhouette_score(muestra, etiquetas_clusters))\n",
    "        except ValueError: # a veces silhouette_score falla en los casos en los que solo hay 1 cluster\n",
    "            pass\n",
    "    puntuacion_media = np.mean(param_resultados)\n",
    "    resultados_busqueda.append([puntuacion_media, param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.75007119325561733, {'eps': 4.9856603649238247, 'min_samples': 6, 'p': 1}],\n",
       " [0.74150194018465343, {'eps': 4.9012580110900821, 'min_samples': 7, 'p': 2}],\n",
       " [0.73747817850020003, {'eps': 4.9832059188723079, 'min_samples': 17, 'p': 2}],\n",
       " [0.72164854282937896, {'eps': 4.5196579162041299, 'min_samples': 9, 'p': 2}],\n",
       " [0.71549187189599339, {'eps': 4.4048553408796298, 'min_samples': 9, 'p': 1}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(resultados_busqueda, key=lambda x: x[0], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_params = {'eps': 4.9856603649238247, 'min_samples':6, 'p': 1}\n",
    "\n",
    "clusterer = DBSCAN(n_jobs=-1, **mejores_params)\n",
    "\n",
    "etiquetas_cluster = clusterer.fit_predict(df_normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    8890\n",
       "-1      60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(etiquetas_cluster).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay 60 anomalias potenciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumen_cluster(cluster_id):\n",
    "    cluster = df[etiquetas_cluster==cluster_id]\n",
    "    resumen_cluster = cluster.mean().to_dict()\n",
    "    resumen_cluster[\"cluster_id\"] = cluster_id\n",
    "    return resumen_cluster\n",
    "\n",
    "def comparar_clusters(*cluster_ids):\n",
    "    resumenes = []\n",
    "    for cluster_id in cluster_ids:\n",
    "        resumenes.append(resumen_cluster(cluster_id))\n",
    "    return pd.DataFrame(resumenes).set_index(\"cluster_id\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster_id</th>\n",
       "      <th>0</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BALANCE</th>\n",
       "      <td>1533.399657</td>\n",
       "      <td>6168.779310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BALANCE_FREQUENCY</th>\n",
       "      <td>0.876730</td>\n",
       "      <td>0.957424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASH_ADVANCE</th>\n",
       "      <td>945.303656</td>\n",
       "      <td>5952.449284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASH_ADVANCE_FREQUENCY</th>\n",
       "      <td>0.134145</td>\n",
       "      <td>0.283207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASH_ADVANCE_TRX</th>\n",
       "      <td>3.136558</td>\n",
       "      <td>19.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CREDIT_LIMIT</th>\n",
       "      <td>4435.728699</td>\n",
       "      <td>13120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INSTALLMENTS_PURCHASES</th>\n",
       "      <td>381.355524</td>\n",
       "      <td>4813.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINIMUM_PAYMENTS</th>\n",
       "      <td>777.355947</td>\n",
       "      <td>9224.292246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONEOFF_PURCHASES</th>\n",
       "      <td>531.828127</td>\n",
       "      <td>9572.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONEOFF_PURCHASES_FREQUENCY</th>\n",
       "      <td>0.200307</td>\n",
       "      <td>0.521089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAYMENTS</th>\n",
       "      <td>1615.261976</td>\n",
       "      <td>19199.308419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRC_FULL_PAYMENT</th>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.289301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASES</th>\n",
       "      <td>912.881443</td>\n",
       "      <td>14386.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASES_FREQUENCY</th>\n",
       "      <td>0.488434</td>\n",
       "      <td>0.774343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASES_INSTALLMENTS_FREQUENCY</th>\n",
       "      <td>0.362509</td>\n",
       "      <td>0.650094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASES_TRX</th>\n",
       "      <td>14.135096</td>\n",
       "      <td>99.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENURE</th>\n",
       "      <td>11.516085</td>\n",
       "      <td>11.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster_id                                  0            -1\n",
       "BALANCE                           1533.399657   6168.779310\n",
       "BALANCE_FREQUENCY                    0.876730      0.957424\n",
       "CASH_ADVANCE                       945.303656   5952.449284\n",
       "CASH_ADVANCE_FREQUENCY               0.134145      0.283207\n",
       "CASH_ADVANCE_TRX                     3.136558     19.883333\n",
       "CREDIT_LIMIT                      4435.728699  13120.000000\n",
       "INSTALLMENTS_PURCHASES             381.355524   4813.413500\n",
       "MINIMUM_PAYMENTS                   777.355947   9224.292246\n",
       "ONEOFF_PURCHASES                   531.828127   9572.707000\n",
       "ONEOFF_PURCHASES_FREQUENCY           0.200307      0.521089\n",
       "PAYMENTS                          1615.261976  19199.308419\n",
       "PRC_FULL_PAYMENT                     0.152800      0.289301\n",
       "PURCHASES                          912.881443  14386.120500\n",
       "PURCHASES_FREQUENCY                  0.488434      0.774343\n",
       "PURCHASES_INSTALLMENTS_FREQUENCY     0.362509      0.650094\n",
       "PURCHASES_TRX                       14.135096     99.866667\n",
       "TENURE                              11.516085     11.700000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparar_clusters(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
